{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# HF Transformers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "#Datahandeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric,list_metrics\n",
    "import datetime\n",
    "\n",
    "# Utils\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# Evaluation\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 15 21:05:14 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 496.13       Driver Version: 496.13       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:25:00.0  On |                  N/A |\n",
      "| 28%   31C    P8     9W / 120W |   1544MiB /  3072MiB |      9%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3308    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A      3360    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      4244    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      4348      C   ...ython\\Python39\\python.exe    N/A      |\n",
      "|    0   N/A  N/A      5960    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7228    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7296    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      7444    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9608    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9732    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     10856    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     10996    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13396    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     13556    C+G   ...\\app-1.0.9004\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     15492    C+G   ...zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     15760    C+G   ...erver\\YourPhoneServer.exe    N/A      |\n",
      "|    0   N/A  N/A     17904    C+G   Insufficient Permissions        N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1060 3GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ds = datasets.load_from_disk(os.path.abspath(\"../data/processed_data\")) #sometimes the relative paths gave my notebook trouble so if that happend I just converted them to absolute paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ARCHITECTURE = 'distilbert-base-uncased' \n",
    "TOKENIZER_PATH = r\"../data/token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(MODEL_ARCHITECTURE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForSequenceClassification.from_pretrained(MODEL_ARCHITECTURE, num_labels=3)\n",
    "\n",
    "def model_init(path=None):\n",
    "    #initializes our model in the Trainer Class later\n",
    "    #See https://discuss.huggingface.co/t/fixing-the-random-seed-in-the-trainer-does-not-produce-the-same-results-across-runs/3442 for more information\n",
    "    if path == None:\n",
    "        return lambda x: AutoModelForSequenceClassification.from_pretrained(MODEL_ARCHITECTURE, num_labels=3) #default mode just returns default model, needs to be wrapped as a function so it works in trainer\n",
    "    else:\n",
    "        return lambda x:AutoModelForSequenceClassification.from_pretrained(path, num_labels=3) # allows to load pretrained models from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9423, 0.2257, 0.8320], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class_weights = (1 - (df[\"class\"].value_counts().sort_index()/len(df))).values #calculate classweights to balance loss function to class imbalance\n",
    "class_weights = torch.from_numpy(class_weights).float().to(\"cuda\") #cass weights into torch.Tensor and push to gpu\n",
    "print(class_weights)\n",
    "\n",
    "class CustomTrainer(Trainer): # Subclassing the Trainer class to overwrite the compute_loss function with our customized loss\n",
    "    #this is the recommended way of doing this, refer to huggingface docs \n",
    "    #LINK https://huggingface.co/docs/transformers/main_classes/trainer#:%7E:text=passed%20at%20init.-,compute_loss,-%2D%20Computes%20the%20loss\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights) # weighting the Crossentropy for our classimbalance\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU - Flushing\n",
    "The Cell below is used for VRAM Flushing so that CUDA Out of Memory errors do not occure or can be solved without restarting the kernel.\n",
    "Sometimes if a cell errors the VRAM can't be appropriately cleared by torch which is when one needs to uncomment the 1/0 to cause an arithmetic error which unlocks the vram to be cleared by torch commands.\n",
    "It's a very useful hack from StackOverflow LINK: https://stackoverflow.com/questions/57858433/how-to-clear-gpu-memory-after-pytorch-model-training-without-restarting-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1/0\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file ../results/model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"../results/last-model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../results/model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ../results/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_metrics(eval_preds): \n",
    "    # alternative compute metric function for general stats\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "    precision = load_metric(\"precision\").compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    f1 = load_metric(\"f1\").compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    recall = load_metric(\"recall\").compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    return {\"accuracy\":accuracy, \"precision\":precision, \"recall\":recall, \"f1\":f1}\n",
    "\n",
    "\n",
    "def compute_hate_metrics(labels:np.array,predictions:np.array)->dict:\n",
    "    # computes recall, precision and f1-score on hate class\n",
    "    gold_hate_mask = (labels == 0) # yields BoolArray with True at index i if the label at index i is the hateclass\n",
    "    gold_non_hate_mask = np.logical_not(gold_hate_mask)\n",
    "    pred_hate_mask = (predictions == 0) # yields BoolArray with True at index i if the prediction at index i is the hateclass\n",
    "    pred_non_hate_mask = np.logical_not(pred_hate_mask)\n",
    "    \n",
    "    tp_mask = gold_hate_mask & pred_hate_mask # True at i if pred = label= True\n",
    "    tn_mask = gold_non_hate_mask & pred_non_hate_mask # True at i if pred = label = False\n",
    "    fp_mask = gold_non_hate_mask & pred_hate_mask # True at i if label != hate but pred = hate\n",
    "    fn_mask = gold_hate_mask & pred_non_hate_mask # True at i if a label = hate but pred != hate\n",
    "    \n",
    "    tp = tp_mask.sum()# sum acts on BooleanArrays by adding 1 for every True entry and 0 for every False thus counting True entries\n",
    "    tn = tn_mask.sum()\n",
    "    fp = fp_mask.sum()\n",
    "    fn = fn_mask.sum()\n",
    "    \n",
    "    recall = tp/(tp+fn)\n",
    "    precision = tp/(tp + fp)\n",
    "    return (recall,precision,(recall + precision)/2)\n",
    "\n",
    "def compute_hatef1(eval_preds):\n",
    "    #computes the f1-score of the hate class\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    hate_recall,hate_precision,hate_f1 = compute_hate_metrics(labels,predictions) \n",
    "    return {\"hatef1\":hate_f1} # just return the hate_f1 since we are mostly interested in this stat\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results\",\n",
    "    seed= 42, #fixing random behaviour\n",
    "    load_best_model_at_end = True, #enable loading the best model at Training end\n",
    "    metric_for_best_model = \"eval_hatef1\", #pick the model with the best hate f1-score\n",
    "    warmup_steps=0,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    #resume_from_checkpoint = \"\" ,\n",
    "    logging_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), #logdir = ..\\logs\\fit\\yyyymmdd-hhmmss\n",
    "    logging_steps = 100,\n",
    "    eval_steps = 500,\n",
    "    #disable_tqdm=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model_init = model_init(path=\"../results/model\"), \n",
    "    # pass model via model init function since otherwise the training is non deterministic even with fixed seed\n",
    "    # since the weights in the Sequential layer are randomly initialized\n",
    "    # See LINK: https://discuss.huggingface.co/t/fixing-the-random-seed-in-the-trainer-does-not-produce-the-same-results-across-runs/3442\n",
    "    # for easier switching between different modes the model_init fct takes a mode and a path and return appropriate models\n",
    "    # if mode = None then it just returns a basic distil-bert-uncased\n",
    "    args=training_args,\n",
    "    train_dataset=train_test_ds[\"train\"],\n",
    "    eval_dataset=train_test_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics = compute_hatef1,\n",
    "    #callbacks = [tb_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../results/last-model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../results/last-model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ../results/last-model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet.\n",
      "***** Running training *****\n",
      "  Num examples = 22304\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2788\n",
      "  4%|▎         | 100/2788 [01:48<45:31,  1.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3004, 'learning_rate': 4.8242467718794835e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 200/2788 [03:37<46:59,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.255, 'learning_rate': 4.644906743185079e-06, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 300/2788 [05:26<45:52,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.255, 'learning_rate': 4.4655667144906746e-06, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 400/2788 [07:14<40:08,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2406, 'learning_rate': 4.28622668579627e-06, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 500/2788 [08:58<38:34,  1.01s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2479\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2495, 'learning_rate': 4.106886657101866e-06, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 18%|█▊        | 500/2788 [09:39<38:34,  1.01s/it]Saving model checkpoint to ../results\\checkpoint-500\n",
      "Configuration saved in ../results\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2681092321872711, 'eval_hatef1': 0.5632429614181439, 'eval_runtime': 40.3779, 'eval_samples_per_second': 61.395, 'eval_steps_per_second': 3.839, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ../results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ../results\\checkpoint-500\\special_tokens_map.json\n",
      " 22%|██▏       | 600/2788 [11:22<36:48,  1.01s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.332, 'learning_rate': 3.927546628407461e-06, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 700/2788 [13:07<36:56,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2798, 'learning_rate': 3.7482065997130563e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 800/2788 [14:57<37:55,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3182, 'learning_rate': 3.5688665710186515e-06, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 900/2788 [16:48<36:04,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2419, 'learning_rate': 3.389526542324247e-06, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1000/2788 [18:39<30:58,  1.04s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2479\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2697, 'learning_rate': 3.2101865136298426e-06, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 36%|███▌      | 1000/2788 [19:19<30:58,  1.04s/it]Saving model checkpoint to ../results\\checkpoint-1000\n",
      "Configuration saved in ../results\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23537857830524445, 'eval_hatef1': 0.5991925947070097, 'eval_runtime': 40.3878, 'eval_samples_per_second': 61.38, 'eval_steps_per_second': 3.838, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ../results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ../results\\checkpoint-1000\\special_tokens_map.json\n",
      " 39%|███▉      | 1100/2788 [21:03<28:20,  1.01s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3478, 'learning_rate': 3.030846484935438e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1200/2788 [22:47<27:44,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2966, 'learning_rate': 2.8515064562410332e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1300/2788 [24:32<26:23,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2451, 'learning_rate': 2.6721664275466288e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1400/2788 [26:16<24:16,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2907, 'learning_rate': 2.492826398852224e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 1500/2788 [28:00<24:20,  1.13s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2479\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2665, 'learning_rate': 2.3134863701578195e-06, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 54%|█████▍    | 1500/2788 [28:44<24:20,  1.13s/it]Saving model checkpoint to ../results\\checkpoint-1500\n",
      "Configuration saved in ../results\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2527189254760742, 'eval_hatef1': 0.5779046155534822, 'eval_runtime': 43.727, 'eval_samples_per_second': 56.693, 'eval_steps_per_second': 3.545, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ../results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ../results\\checkpoint-1500\\special_tokens_map.json\n",
      " 57%|█████▋    | 1600/2788 [30:33<20:16,  1.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2183, 'learning_rate': 2.1341463414634146e-06, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 1700/2788 [32:18<18:58,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2584, 'learning_rate': 1.95480631276901e-06, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 1800/2788 [34:02<17:05,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2482, 'learning_rate': 1.7772596843615496e-06, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1900/2788 [35:44<15:05,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2153, 'learning_rate': 1.597919655667145e-06, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 2000/2788 [37:29<15:12,  1.16s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2479\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2301, 'learning_rate': 1.4185796269727403e-06, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 72%|███████▏  | 2000/2788 [38:13<15:12,  1.16s/it]Saving model checkpoint to ../results\\checkpoint-2000\n",
      "Configuration saved in ../results\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25007203221321106, 'eval_hatef1': 0.5692690866048531, 'eval_runtime': 43.9879, 'eval_samples_per_second': 56.356, 'eval_steps_per_second': 3.524, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ../results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ../results\\checkpoint-2000\\special_tokens_map.json\n",
      " 75%|███████▌  | 2100/2788 [40:04<12:27,  1.09s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2104, 'learning_rate': 1.2392395982783357e-06, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 2200/2788 [41:54<10:13,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1852, 'learning_rate': 1.0598995695839312e-06, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 2300/2788 [43:39<08:36,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2829, 'learning_rate': 8.805595408895266e-07, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 2400/2788 [45:25<07:06,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3338, 'learning_rate': 7.012195121951221e-07, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 2500/2788 [47:16<05:16,  1.10s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2479\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2786, 'learning_rate': 5.218794835007174e-07, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|████████▉ | 2500/2788 [48:00<05:16,  1.10s/it]Saving model checkpoint to ../results\\checkpoint-2500\n",
      "Configuration saved in ../results\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2494821697473526, 'eval_hatef1': 0.580919615044865, 'eval_runtime': 43.6504, 'eval_samples_per_second': 56.792, 'eval_steps_per_second': 3.551, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ../results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ../results\\checkpoint-2500\\special_tokens_map.json\n",
      " 93%|█████████▎| 2600/2788 [49:51<03:23,  1.08s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2752, 'learning_rate': 3.425394548063128e-07, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2700/2788 [51:43<01:35,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2743, 'learning_rate': 1.6319942611190818e-07, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2788/2788 [53:20<00:00,  1.11s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../results\\checkpoint-1000 (score: 0.5991925947070097).\n",
      "100%|██████████| 2788/2788 [53:20<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3200.8447, 'train_samples_per_second': 13.936, 'train_steps_per_second': 0.871, 'train_loss': 0.265225251743749, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2788, training_loss=0.265225251743749, metrics={'train_runtime': 3200.8447, 'train_samples_per_second': 13.936, 'train_steps_per_second': 0.871, 'train_loss': 0.265225251743749, 'epoch': 2.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"../results/training_begin=\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") #save the modelpath when training starts\n",
    "trainer.train() # start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18724/4051488549.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# save best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# save tokenizer for model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#cd to modeldir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training_args\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#write the training args into the directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_model(model_path) # save best model\n",
    "tokenizer.save_pretrained(model_path) # save tokenizer for model\n",
    "os.chdir(model_path) #cd to modeldir\n",
    "with open(\"training_args\", \"wb\") as output_file:\n",
    "    pickle.dump(training_args, output_file) #write the training args into the directory\n",
    "os.chdir(\"../../src\") #cd back to initial working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Evaluation of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2479\n",
      "  Batch size = 16\n",
      "100%|██████████| 155/155 [00:43<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2479, 3) (2479,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(train_test_ds[\"test\"]) \n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(predictions.predictions, axis=-1) #get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.66       134\n",
      "           1       0.97      0.96      0.96      1899\n",
      "           2       0.96      0.95      0.96       446\n",
      "\n",
      "    accuracy                           0.94      2479\n",
      "   macro avg       0.84      0.89      0.86      2479\n",
      "weighted avg       0.95      0.94      0.95      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=predictions.label_ids,y_pred = y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_names = (\"hate\",\"offensive\",\"neither\")\n",
    "cm = confusion_matrix(y_true=predictions.label_ids,y_pred=y_preds)\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cccfd8fa3692e38c9904a15873ca7e5836ef9bf62e6240950a461422140e1b82"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
