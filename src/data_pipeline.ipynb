{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch + HF Transformers\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Data Handeling\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "# Text Cleaning\n",
    "import spacy \n",
    "\n",
    "# OS Utils\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if GPU is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  7 14:17:49 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 496.13       Driver Version: 496.13       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:25:00.0  On |                  N/A |\n",
      "| 28%   30C    P8    10W / 120W |   3010MiB /  3072MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1168    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      2948    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      4020    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A      5984    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6812    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      7852    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A      8036    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8464    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9140    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10008    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     10356    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     11620    C+G   ...erver\\YourPhoneServer.exe    N/A      |\n",
      "|    0   N/A  N/A     11724    C+G   ...\\app-1.0.9004\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     12080    C+G   ...zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     12192    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13100    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     14404      C   ...ython\\Python39\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1060 3GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load local Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/labeled_data.csv\") #load local data stored as csv\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df) # convert pandas dataframe into Huggingface dataset for later use of the Trainer() API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTITY IDS =  ('url', 'ment', 'ws')\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "ruler = nlp.add_pipe(\"entity_ruler\",config = {\"overwrite_ents\": True}) # Overwrite predefined Ents so only the below specified patterns are recognized\n",
    "\n",
    "# general structure of a spacy pattern is {label:LABEL_NAME, pattern = [{pattern_type:pattern_string},{},...{}] list of subpatterns, id:ID_STRING} \n",
    "patterns = [\n",
    "                {\"label\": \"[URL]\", \"pattern\": [{\"TEXT\":{\"REGEX\":'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'}}], \"id\":\"url\"},\n",
    "                {\"label\": \"[MENTION]\", \"pattern\": [ {\"TEXT\":{\"REGEX\":\"@[\\w\\-]+\"}}], \"id\":\"ment\"},\n",
    "                {\"label\": \"[WHITESPACE]\", \"pattern\": [ {\"TEXT\":{\"REGEX\":\"\\s+\"}}], \"id\":\"ws\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns) # add patterns to ruler-object\n",
    "print(\"ENTITY IDS = \",ruler.ent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(text_string): \n",
    "    doc = nlp(text_string)\n",
    "    out_string = \"\"\n",
    "    for token in doc: # iterate over all tokens in the document\n",
    "        if token.ent_id_ == \"\": # is the token an entitiy of Null-Type then lemmatize it\n",
    "            out_string = out_string + \" \" + token.lemma_\n",
    "        else:\n",
    "            if token.ent_id_ == \"ws\": # is the token just whitespace then forget the token\n",
    "                pass\n",
    "            else: # otherwise the token has to be a mention or a url so just append the name of the entity-type\n",
    "                out_string = out_string + \" \" + token.ent_type_\n",
    "    return out_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_STR:\n",
      " this is a beautiful teststring                               hey @yourlocaltwitteruser \n",
      "look at all this whitespace, i hate it, can you please google how to remove it? Use https://www.google.de \n",
      "\n",
      "\n",
      "PROCESSED:\n",
      "  this be a beautiful teststring hey [MENTION] look at all this whitespace , I hate it , can you please google how to remove it ? use [URL]\n"
     ]
    }
   ],
   "source": [
    "test_str = \"\"\"this is a beautiful teststring                               hey @yourlocaltwitteruser \n",
    "look at all this whitespace, i hate it, can you please google how to remove it? Use https://www.google.de \"\"\"\n",
    "\n",
    "print(\"TEST_STR:\\n\",test_str)\n",
    "print(\"\\n\")\n",
    "print(\"PROCESSED:\\n\",preprocess_tweet(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased') #load correct tokenizer for uncased distilbert\n",
    "\n",
    "MAX_SEQ_LEN = 128 # tweets are rarely over 128 tokens long\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token) # adding padding\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token) # adding UNKNOWN-token for out of vocabulary situations\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    processed_tweets = [preprocess_tweet(tweet) for tweet in data[\"tweet\"]] # preprocess all tweets in the data\n",
    "    return tokenizer(processed_tweets,padding = \"max_length\",max_length=128,truncation=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:58<00:00,  7.13s/ba]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function,batched = True) # applying the map method allows for very quick batched processing of text data\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"class\",\"labels\") # Huggingface Trainier expects naming conventions thus rename everything accordingly\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"hate_speech\",\"offensive_language\",\"neither\",\"count\",\"Unnamed: 0\"]) # remove unneccessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10988/3954752550.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10988/3227637127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_test_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# split in train test sets with test_size = 10%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_test_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_test_ds = tokenized_dataset.train_test_split(test_size=0.1) # split in train test sets with test_size = 10%\n",
    "print(train_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_PATH = r\"../data/processed_data\"\n",
    "\n",
    "if os.path.isdir(DATA_PATH):\n",
    "    print(\"File already exists ,overwriting file...\")\n",
    "    shutil.rmtree(DATA_PATH) # this is neccesary since the huggingface method has no way of overwriting the given directory which will lead to OSError22\n",
    "    train_test_ds.save_to_disk(r\"../data/processed_data\")\n",
    "    print(\"File saved at \" + os.path.abspath(DATA_PATH))\n",
    "else:\n",
    "    train_test_ds.save_to_disk(r\"../data/processed_data\")\n",
    "    print(\"File saved at \" + os.path.abspath(DATA_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10988/3029214333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\MEGAPORT\\\\Documents\\\\Uni\\\\WiSe2021_22\\\\ProjektSeminar\\\\Project\\\\H8-Detect\\\\data\\\\processed_daa'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cccfd8fa3692e38c9904a15873ca7e5836ef9bf62e6240950a461422140e1b82"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
